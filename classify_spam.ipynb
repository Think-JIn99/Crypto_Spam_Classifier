{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스팸 게시글 구분을 위한 SVM 만들기\n",
    "* 스팸 게시글을 구분하기 위한 SVM 구축을 목적으로 합니다.\n",
    "* 기본적인 SVM 구축을 위한 데이터 전처리 및 단어 벡터화의 내용을 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nlt\n",
    "import os\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./dataset/train/train.csv', index_col = 0)\n",
    "train_X, train_y = train_df['title'].str.lower(), train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        bitcoin mvrv drops below  as investors deposit...\n",
       "1        change your life lpn token register now. https...\n",
       "2        saw this upsetting comment on a financial depr...\n",
       "3        for guaranteed investment and resolve of compl...\n",
       "4                 crypto tax calculator | cryptotrader.tax\n",
       "                               ...                        \n",
       "23608    dumb question if people cant afford to buy a f...\n",
       "23609    miss the last bull run dont dismay relive all ...\n",
       "23610    how to profit from the  cryptocurrency bullrun...\n",
       "23611    bitcoin market insights webinar - bitcoin indu...\n",
       "23612                           billion market cap reached\n",
       "Name: title, Length: 23613, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특수 문자 및 이모지를 텍스트로 치환해준다.\n",
    "* url의 경우 httpadr로 변환한다.\n",
    "* 이모지의 경우 emot를 사용해 텍스트로 변환한다.\n",
    "* $나 !의 경우 각각 dollar 와 exclamation으로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_regex(title_series):\n",
    "    url_regex = \"(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}\"\n",
    "    title_series = title_series.str.replace(url_regex, 'httpaddr',regex=True)\n",
    "    title_series = title_series.str.replace('[$]+','dollar',regex=True)\n",
    "    title_series = title_series.str.replace('[!]+','excalmation',regex=True)\n",
    "    title_series = title_series.str.replace('[0-9]+','number',regex=True)\n",
    "    return title_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##이모티콘을 텍스트로 치환해준다.\n",
    "def replace_emoji(title_series): #토큰화를 진행한 후 각 토큰에 존재하는 이모지들을 텍스트로 치환해준다.\n",
    "    import emot\n",
    "    emot_obj = emot.core.emot()\n",
    "    for v in title_series:\n",
    "        emoji = emot_obj.emoji(v)\n",
    "        if emoji['value']:\n",
    "            v += ' '.join(emoji['value'])\n",
    "    title_series = title_series.str.replace('[^a-zA-Z ]','',regex=True) #불용어 전부 제거\n",
    "    return title_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = replace_regex(train_X)\n",
    "train_X = replace_emoji(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불필요한 기호나 부호 이모지를 제거해준다.\n",
    "* 이모지 제거\n",
    "* 특수기호 제거\n",
    "* 숫자 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 토큰화와 표제어 추출\n",
    "* 앞서 우리는 토큰화에 앞서 처리해야 하는 특수 단어들을 대체 했다.\n",
    "* 이제 문장들을 토큰 단위로 쪼개고 하나의 표제어로 묶는 작업을 진행해 보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(title_sereis):\n",
    "    tokenizer = nlt.tokenize.TreebankWordTokenizer()\n",
    "    tokenized_data = title_sereis.apply(tokenizer.tokenize) #트리뱅크 토크나이저로 문장을 토큰화 해준다.\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentences):\n",
    "    #표제어 추출에 앞서 각 토큰에 품사를 결정해준다.\n",
    "    #품사를 미리 지정해 줌으로써 표제어 추출의 정확도를 향상 시킬 수 있다.\n",
    "    pos_list = [nlt.pos_tag(token) for token in sentences]\n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = pos_tag(tokenize(train_X)) #품사가 지정된 토큰 리스트를 반환 받는다.\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(pos_list):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() #표제어 추출기 \n",
    "    lemmatized_res = []\n",
    "    for sentece in pos_list:\n",
    "        temp = [] #한 문장에서 추출된 표제어를 임시로 저장한다.\n",
    "        for token, pos in sentece:\n",
    "            #lemmatize 함수의 매개변수는 v,a,n,r로 동 형 명 부 사 밖에 없기 때문에 다시금 매핑을 진행해야 한다.\n",
    "            if token not in stopwords.words('english') and pos[0] in ['V','J','N','R']: \n",
    "                #불용어가 아니고 동,형,명,부사의 한 종류일 경우 표제어 추출을 진행한다.\n",
    "                #j는 a로 취급해줘야 한다. lemmtizer는 형용사를 pos와 다른 알파벳으로 취급한다.\n",
    "                _pos = 'a' if pos[0] == 'J' else pos.lower()[0] \n",
    "                temp.append((token,_pos))\n",
    "        lemmatized_res.append([lemmatizer.lemmatize(t,p) for t,p in temp]) #표제어를 추출한다.\n",
    "    return lemmatized_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = lemmatize(pos_list) #표제어가 추출된 토큰 리스트\n",
    "lemmatized_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bitcoin, mvrv, drop, investor, deposit, b, bt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[change, life, lpn, token, register, httpaddrr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[saw, upsetting, comment, financial, depressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[guarantee, investment, resolve, complaint, co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[crypto, tax, calculator, httpaddr]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23608</th>\n",
       "      <td>[dumb, question, people, cant, afford, buy, fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23609</th>\n",
       "      <td>[miss, last, bull, run, dont, dismay, relive, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23610</th>\n",
       "      <td>[profit, cryptocurrency, bullrun, potentially,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23611</th>\n",
       "      <td>[bitcoin, market, insight, webinar, bitcoin, i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23612</th>\n",
       "      <td>[market, cap, reach]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens label\n",
       "0      [bitcoin, mvrv, drop, investor, deposit, b, bt...     0\n",
       "1      [change, life, lpn, token, register, httpaddrr...     1\n",
       "2      [saw, upsetting, comment, financial, depressio...     0\n",
       "3      [guarantee, investment, resolve, complaint, co...     1\n",
       "4                    [crypto, tax, calculator, httpaddr]     1\n",
       "...                                                  ...   ...\n",
       "23608  [dumb, question, people, cant, afford, buy, fu...     0\n",
       "23609  [miss, last, bull, run, dont, dismay, relive, ...     0\n",
       "23610  [profit, cryptocurrency, bullrun, potentially,...     0\n",
       "23611  [bitcoin, market, insight, webinar, bitcoin, i...     1\n",
       "23612                               [market, cap, reach]     0\n",
       "\n",
       "[23613 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_df = pd.DataFrame([lemmatized_tokens, train_y]).T\n",
    "lemma_df.columns = ['tokens','label'] \n",
    "lemma_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표제어 추출의 결과\n",
    "* 단어를 토큰화하고 표제어 추출을 진행하니 문장의 원초적인 요소들만이 남은 것을 확인 할 수 있다.\n",
    "* 이제 불용어 및 빈도수가 낮은 단어들을 처내고 사용할 단어들을 선별하는 작업을 진행해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 잠깐! 이런 단어들은 어쩌지?\n",
    "* 우리는 게시글 데이터를 가격과 연관이 있는 게시글 그리고 가격과 연관이 없는 게시글로 분류를 하고 있다.\n",
    "* 이때 두 종류의 글에서 동시에 자주 등장하는 단어의 경우 어떻게 처리해야 할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequent_sereis(lemmatized_tokens, freq_bound = 50, word_length = 2):\n",
    "    #각 단어의 빈도수를 추출한다.\n",
    "    word_freq = pd.Series(np.concatenate([w for w in lemmatized_tokens])).value_counts()\n",
    "    words_available = word_freq.loc[(word_freq > freq_bound)] #10회 이상 사용된 단어로만\n",
    "    indices = pd.Series(words_available.index)\n",
    "    #단어의 길이가 2 이상인 단어들로\n",
    "    indices = indices.loc[indices.apply(len) > word_length]\n",
    "    freq_series = words_available[indices] #사용할 단어들만을 모아 단어,빈도수 딕셔너리를 생성하자\n",
    "    return freq_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doubled_words(lemma_df):\n",
    "    neg_df = lemma_df.groupby('label').get_group(0)\n",
    "    pos_df = lemma_df.groupby('label').get_group(1)\n",
    "    pos_lemma, neg_lemma = pos_df['tokens'], neg_df['tokens']\n",
    "    pos_freq = create_frequent_sereis(pos_lemma)\n",
    "    neg_freq = create_frequent_sereis(neg_lemma)\n",
    "    doubled_words = [i for i in pos_freq.index if i in neg_freq.index]\n",
    "    return doubled_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 겹치는 단어들을 살펴보니\n",
    "* 겹치는 단어들의 모습을 살펴보니 우리가 정말로 어떤 형식의 글에서든 존재할 수 밖에 없는 단어들이었다.\n",
    "* 해당 단어들은 데이터의 성격상 늘상 존재할 수 밖에 없는 단어 이므로 특정 데이터의 성질을 판단하는데 사용할 수 없다.\n",
    "* 어디에나 있는 것으로 판단할 수 없기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doubled_words(freq_series, words):\n",
    "    for w in words:\n",
    "        try:\n",
    "            freq_series.drop(w, inplace = True)\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rank_series(freq_series):\n",
    "    freq_rank = []\n",
    "    rank = 1\n",
    "    before = freq_series[0]\n",
    "    for f in freq_series:\n",
    "        if f == before: #빈도수가 동일하면 동일한 랭크를 갖는다.\n",
    "            freq_rank.append(rank)\n",
    "        else:\n",
    "            rank += 1\n",
    "            freq_rank.append(rank)\n",
    "            before = f\n",
    "    return freq_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_series = create_frequent_sereis(lemmatized_tokens, 300)\n",
    "remove_doubled_words(freq_series, get_doubled_words(lemma_df))\n",
    "freq_dict = {i:r for i,r in zip(freq_series.index, create_rank_series(freq_series))}\n",
    "freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어의 벡터화\n",
    "* 이제 앞서 만든 빈도수 사전을 통해 문장을 벡터로 표현해보자.\n",
    "* 각 단어는 문자 대신 빈도수 형태로 표현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens, freq_dict):\n",
    "    vectorized = []\n",
    "    max_length = max(map(len,tokens)) #가장 긴 문장의 단어 개수\n",
    "    for sentence in tokens:\n",
    "        vector = [freq_dict[word] if word in freq_dict.keys() else 0 for word in sentence]\n",
    "        vector.extend([0] * (max_length - len(vector)))\n",
    "        vectorized.append(vector)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0889647, -0.0889647, -0.0889647, ..., -0.0889647, -0.0889647,\n",
       "        -0.0889647],\n",
       "       [-0.0889647, -0.0889647, -0.0889647, ..., -0.0889647, -0.0889647,\n",
       "        -0.0889647],\n",
       "       [-0.0889647, -0.0889647, -0.0889647, ..., -0.0889647, -0.0889647,\n",
       "        -0.0889647],\n",
       "       ...,\n",
       "       [-0.0889647, -0.0889647, -0.0889647, ..., -0.0889647, -0.0889647,\n",
       "        -0.0889647],\n",
       "       [-0.0889647, -0.0889647, -0.0889647, ..., -0.0889647, -0.0889647,\n",
       "        -0.0889647],\n",
       "       [-0.0889647, -0.0889647, -0.0889647, ..., -0.0889647, -0.0889647,\n",
       "        -0.0889647]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.array(vectorize(lemmatized_tokens, freq_dict))\n",
    "vectors = (vectors - vectors.mean()) / (vectors.std())\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 마침내!! 드디어!!\n",
    "> 우리는 이제야 데이터 전처리 과정을 마쳤다. 긴 여정이었다. 지나온 길을 빠르게 돌아보자.\n",
    "1. 결측치 제거\n",
    "2. 이모티콘, 느낌표, 달러 등 유의미한 특수기호 텍스트로 변환\n",
    "3. 의미 없는 이모지, 특수기호, 숫자등 알파벳이 아닌 문자 전부 제거\n",
    "4. 토크나이징\n",
    "5. 토큰에 품사 태깅\n",
    "6. 품사 태깅된 토큰 리스트를 사용해 표제어 추출\n",
    "7. 표제어 중에서 불용어 제거 후 각 단어별 빈도수 순위 사전 생성\n",
    "8. 사전에 존재하는 순위를 활용해 각 표제어 토큰 리스트를 랭킹 정보를 갖고 있는 정수 벡터로 치환\n",
    "9. 정수 벡터 리스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이젠 모델링이 하고 싶어요..\n",
    "* 선생님.. 모델링이 하고 싶어요..\n",
    "* 먼길 돌아왔다. 이젠 모델링을 진행해보자.\n",
    "* 우리는 SVM을 사용해 분류를 진행할 것이다.\n",
    "* 학습,검증,테스트를 6:2:2 비율로 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 평가를 위한 평가지표인 f1 스코어를 정의 해준다.\n",
    "def get_f1(prediction, label_data, C):\n",
    "    pos_true = label_data.loc[(prediction == 1) & (label_data == prediction)].size\n",
    "    pos_false = label_data.loc[(prediction == 1) & (label_data != prediction)].size\n",
    "    neg_false = label_data.loc[(prediction == 0) & (label_data != prediction)].size\n",
    "    precision = pos_true / (pos_true + pos_false)\n",
    "    recall = pos_true / (pos_true + neg_false)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = accuracy_score(prediction, label_data) * 100\n",
    "    print(f'precision:{precision}, recall:{recall}, f1:{f1_score}')\n",
    "    print(f'accuracy:{accuracy}')\n",
    "    return {'C': C, 'accuracy': accuracy, 'recall': recall, 'precision': precision, 'F1-score': f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_f1() missing 1 required positional argument: 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f87a5dc6299e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_f1() missing 1 required positional argument: 'C'"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C = 0.3, kernel = 'linear', degree = 2, gamma = 'auto')\n",
    "SVM.fit(vectors, train_y)\n",
    "prediction = SVM.predict(vectors)\n",
    "get_f1(prediction, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정확도 증가를 위한 새로운 방향\n",
    "* 정확도 자체는 나쁘지 않지만, 리콜 값이 무척이나 밑돌고 있는 점을 확인 할 수 있다.\n",
    "* 이는 모델이 neg_false를 많이 도출하고 있다는 의미이며, 이는 스팸이 아닌 메일을 스팸으로 분류하는 경우가 다분하다는 뜻이다.\n",
    "* 벡터라이징 기법에 문제가 있는 것은 아닐까? \n",
    "* 단순히 빈도수를 통한 순위가 아닌 조금 더 수학적인 기법을 활용해 벡터라이징을 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidfvec(tokens, freq_dict):\n",
    "       #필터를 통해 유의미한 단어만 문장에서 걸러준다.\n",
    "       tfidfv_words = [list(filter(lambda x: x in freq_dict.keys(), sentence)) for sentence in tokens]\n",
    "       #해당 단어가 한개도 존재하지 않는 문장일 경우 공백으로 표시해준다.\n",
    "       corpus = list(map(lambda x: ' '.join(x) if len(x) > 0 else \" \", tfidfv_words))\n",
    "       tfidfv = TfidfVectorizer().fit(corpus)\n",
    "       return tfidfv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23613, 15)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfv = get_tfidfvec(lemma_df['tokens'], freq_dict)\n",
    "tfidfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.9954582989265071, recall:0.7942024211479864, f1:0.8835142687004718\n",
      "accuracy:89.23050861813408\n"
     ]
    }
   ],
   "source": [
    "#tf-id 벡터라이징을 적용한 SVM\n",
    "SVM = svm.SVC(C = 0.3, kernel = 'linear', degree = 2, gamma = 'auto')\n",
    "SVM.fit(tfidfv, train_y)\n",
    "prediction = SVM.predict(tfidfv)\n",
    "get_f1(prediction, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-ID 기법이 확실히 더 나은 성능을 보인다.\n",
    "* 벡터라이징도 짱구를 굴려서 작업을 해야한다.\n",
    "* 자세한 사항은 링크를 통해 학습하자.\n",
    "* https://wikidocs.net/31698"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검증셋을 통해 최적의 C와 감마 등을 탐색해보자.\n",
    "* 검증 셋을 활용해 가장 결과가 괜찮은 파라미터 수치를 설정하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#앞 선 전처리 과정을 한번에 진행해주는 함수\n",
    "def preprocess(X, freq_dict, tfidfv = True):\n",
    "    X_data = replace_regex(X)\n",
    "    X_data = replace_emoji(X_data)\n",
    "    tokenized_data = tokenize(X_data)\n",
    "    pos_list = pos_tag(tokenized_data)\n",
    "    tokens = lemmatize(pos_list)\n",
    "    #표제어 데이터 프레임 생성\n",
    "\n",
    "    #tfid를 사용한다.\n",
    "    if tfidfv:\n",
    "        vectorized = get_tfidfvec(tokens,freq_dict)\n",
    "    else:\n",
    "        vectorized = np.array(vectorize(tokens, freq_dict))\n",
    "        vectorized = (vectorized - vectorized.mean()) / (vectorized.std())\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.9939277724512624, recall:0.784957092377587, f1:0.877168241432802\n",
      "accuracy:88.93406174564859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 88.93406174564859,\n",
       " 'recall': 0.784957092377587,\n",
       " 'precision': 0.9939277724512624,\n",
       " 'F1-score': 0.877168241432802}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.read_csv('./dataset/train/valid.csv',index_col = 0)\n",
    "valid_X, valid_y = valid_df['title'].str.lower(), valid_df['label']\n",
    "vectors = preprocess(valid_X, freq_dict)\n",
    "SVM = svm.SVC(C = 0.3, kernel = 'linear', degree = 2, gamma = 'auto')\n",
    "SVM.fit(vectors, valid_y)\n",
    "valid_prediction = SVM.predict(vectors)\n",
    "f1 = get_f1(valid_prediction, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(SVM, vectors, y):\n",
    "    y_range = [0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "    df = pd.DataFrame()\n",
    "    for c_value in y_range:\n",
    "        SVM = svm.SVC(C = c_value, kernel = 'linear', degree = 2, gamma = 'auto')\n",
    "        SVM.fit(vectors, y)\n",
    "        prediction = SVM.predict(vectors)\n",
    "        df = df.append(get_f1(prediction, y, c_value), ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.993747943402435, recall:0.762241292276628, f1:0.8627338951578346\n",
      "accuracy:87.79062380891881\n",
      "precision:0.9939277724512624, recall:0.784957092377587, f1:0.877168241432802\n",
      "accuracy:88.93406174564859\n",
      "precision:0.9939277724512624, recall:0.784957092377587, f1:0.877168241432802\n",
      "accuracy:88.93406174564859\n",
      "precision:0.9939277724512624, recall:0.784957092377587, f1:0.877168241432802\n",
      "accuracy:88.93406174564859\n",
      "precision:0.9939277724512624, recall:0.784957092377587, f1:0.877168241432802\n",
      "accuracy:88.93406174564859\n",
      "precision:0.9939277724512624, recall:0.784957092377587, f1:0.877168241432802\n",
      "accuracy:88.93406174564859\n"
     ]
    }
   ],
   "source": [
    "res = optimize(SVM, vectors, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.877168</td>\n",
       "      <td>88.934062</td>\n",
       "      <td>0.993928</td>\n",
       "      <td>0.784957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.877168</td>\n",
       "      <td>88.934062</td>\n",
       "      <td>0.993928</td>\n",
       "      <td>0.784957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.877168</td>\n",
       "      <td>88.934062</td>\n",
       "      <td>0.993928</td>\n",
       "      <td>0.784957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.877168</td>\n",
       "      <td>88.934062</td>\n",
       "      <td>0.993928</td>\n",
       "      <td>0.784957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.877168</td>\n",
       "      <td>88.934062</td>\n",
       "      <td>0.993928</td>\n",
       "      <td>0.784957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C  F1-score   accuracy  precision    recall\n",
       "1  0.03  0.877168  88.934062   0.993928  0.784957\n",
       "2  0.10  0.877168  88.934062   0.993928  0.784957\n",
       "3  0.30  0.877168  88.934062   0.993928  0.784957\n",
       "4  1.00  0.877168  88.934062   0.993928  0.784957\n",
       "5  3.00  0.877168  88.934062   0.993928  0.784957"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc[res['F1-score'].max() == res['F1-score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최후의 테스트 진행해보기.\n",
    "* 우리는 지금까지 텍스트의 전처리 벡터라이징을 진행한 뒤 모델을 학습시켰고\n",
    "* 이후 모델의 파라미터를 최적화 하기 위해 optimize 함수를 활용해 최적화된 C값을 찾아봤다.\n",
    "* 이제 최적화 된 C값을 통해 모델의 최종 결과를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.9965222889661713, recall:0.7868197703444832, f1:0.879341609708467\n",
      "accuracy:89.01029094143057\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('./dataset/train/test.csv',index_col = 0)\n",
    "test_X, test_y = test_df['title'].str.lower(), test_df['label']\n",
    "vectors = preprocess(test_X, freq_dict)\n",
    "SVM = svm.SVC(C = 0.03, kernel = 'linear', degree = 2, gamma = 'auto')\n",
    "SVM.fit(vectors, test_y)\n",
    "test_prediction = SVM.predict(vectors)\n",
    "f1 = get_f1(test_prediction, test_y, 0.03)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67a9d45133f1a9e4e6de0dcd9787e83ef7d08e0d5bdd34045d03563c32d8ac89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('py_3.8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
